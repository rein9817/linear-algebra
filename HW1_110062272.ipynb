{"cells":[{"cell_type":"markdown","metadata":{"id":"5N5RuFIhveel"},"source":["# Homework assignment 1: Markov Chain"]},{"cell_type":"markdown","metadata":{"id":"H1UaCc1Qveen"},"source":["## Problem 1\n","Read the article in https://www.geeksforgeeks.org/markov-chains-in-nlp/, and answer the following questions."]},{"cell_type":"markdown","metadata":{"id":"CBof0iUZveeo"},"source":["* (a) What are N-grams of an input sequence?  \n","        A contiguous sequence of n items (words or characters) from a given sample of text.\n","* (b) How to determine the probability of each element in the transition matrix?  \n","        Each element (i,j) of the matrix represents the probability of moving from state i to state j.\n","        For example, if you’re in state 1 and want to move to state 2 or state 3, you would look at the row for state 1 and the columns for state 2 and state 3 to find the probabilities.\n","* (c) If you want to increase the variety of the sequence generation (every time the outputs are different), what kinds of properties the training texts should be?  \n","        The probabilities of moving from one word to another should be balanced."]},{"cell_type":"markdown","metadata":{"id":"IW3fVQRdveeo"},"source":["## Problem 2\n","Try the following codes, and answer questions."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Install the required packages\n","#!pip install nltk\n","# !pip install numpy"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2579,"status":"ok","timestamp":1725465452265,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"3rvAjDpaveep"},"outputs":[],"source":["import re\n","from nltk import ngrams\n","import numpy as np\n","from typing import List, Tuple, Union, Dict\n","from itertools import product"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":443,"status":"ok","timestamp":1725465464133,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"fuUPhTVWveeq"},"outputs":[],"source":["def words_to_index(words: List[str], base: int, unique_words: Dict[str, int]) -> int:\n","    \"\"\"\n","        Convert a list of words to an index representation.\n","\n","        Args:\n","            words (List[str]): The list of words to be converted.\n","            base (int): The base value used for conversion.\n","            unique_words (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n","\n","        Returns:\n","            - int: The index representation of the given list of words.\n","        \n","        Example:\n","            >>> words_to_index([\"the\", \"quick\", \"brown\", \"fox\"], 10, {\"the\": 0, \"quick\": 1, \"brown\": 2, \"fox\": 3})\n","            123\n","    \"\"\"\n","    length = len(words)\n","    numbers = [unique_words[word] for word in words]\n","    return sum([num * (base ** (length - 1 - idx)) for idx, num in enumerate(numbers)])\n","\n","def index_to_words(index: int, base: int, length: int, unique_words: Dict[str, int]):\n","    \"\"\"\n","        Converts an index to a list of words based on a given base and unique words.\n","\n","        Args:\n","            index (int): The index to convert.\n","            base (int): The base used for conversion.\n","            length (int): The length of the resulting list.\n","            unique_words (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n","\n","        Returns:\n","            List[str]: A list of words corresponding to the given index.\n","\n","        Examples:\n","            >>> unique_words = {'apple': 0, 'banana': 1, 'cherry': 2}\n","            >>> index_to_words(5, 3, 2, unique_words)\n","            ['banana', 'cherry'] (Because 5 = 1 * 3^1 + 2 * 2^0)\n","\n","            >>> unique_words = {'red': 0, 'green': 1, 'blue': 2}\n","            >>> index_to_words(2, 3, 1, unique_words)\n","            ['blue'] (Because 2 = 2 * 3^0)\n","    \"\"\"\n","    numbers = []\n","    \n","    unique_words_list = list(unique_words.keys())\n","    for pow in range(length - 1, -1, -1):\n","        numbers.append(index // (base ** pow))\n","        index -= numbers[-1] * (base ** pow)\n","    \n","    return [unique_words_list[num] for num in numbers]"]},{"cell_type":"markdown","metadata":{"id":"3q89RN21veer"},"source":["### Step 1: Remove some unnecessary characters"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":447,"status":"ok","timestamp":1725465482890,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"-bLwYWvDveer"},"outputs":[],"source":["def remove_unnecessary_characters(text: str):\n","    \"\"\"\n","        Removes unnecessary characters from the given text and converts it to lowercase.\n","\n","        Args:\n","            text (str): The input text to be processed.\n","        Returns:\n","            str: The processed text with unnecessary characters removed and converted to lowercase.\n","    \"\"\"\n","    # Remove unnecessary characters\n","    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"dDPRDP_Dveer"},"source":["### Step 2: Generate N-grams"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":570,"status":"ok","timestamp":1725465486350,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"_XqvOwjOvees"},"outputs":[],"source":["def generate_n_grams(text: str, n: int):\n","    \"\"\"\n","        Generate n-grams from the given text.\n","\n","        Args:\n","            text (str): The input text from which n-grams will be generated.\n","            n (int): The number of consecutive words in each n-gram.\n","        Returns:\n","            List[Tuple[str]]: A list of tuples representing the generated n-grams.\n","    \"\"\"\n","    # [TODO] Generate n-grams, using the ngrams function from the nltk library\n","    n_grams = ngrams(text.split(),n)\n","    print(\"n grams\",n_grams)\n","    # Convert to list of tuples\n","    n_grams = list(n_grams)\n","\n","    return n_grams"]},{"cell_type":"markdown","metadata":{"id":"RLrCGQuyvees"},"source":["### Step 3: Compute Transition Matrix"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":536,"status":"ok","timestamp":1725465500278,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"ij_Pxsn6vees"},"outputs":[],"source":["def compute_transition_matrix(n_grams: List[Tuple[str]]):\n","    \"\"\"\n","        Computes the transition matrix and unique states for a given list of n-grams.\n","            \n","        Args:\n","            n_grams (List[Tuple[str]]): A list of n-grams, where each n-gram is a tuple of strings.\n","        Returns:\n","            np.ndarray: transition matrix.\n","            Dict[str, int]: A dictionary mapping unique words to their corresponding indices.\n","    \"\"\"\n","    \n","    # Get the value of n\n","    n = len(n_grams[0])\n","    \n","    ## Step 3.1: Collect all possible words (label words with indices)\n","    \n","    unique_words = {}\n","    unique_words_count = 0\n","    \n","    # A helper function to add a word to the unique_words dictionary, if it is not already present\n","    def _add_to_unique_word(word: str):\n","        # We need to increase the counter outside this function\n","        nonlocal unique_words_count\n","\n","        # Check if the state is already in the unique_states dictionary\n","        if unique_words.get(word, None) is None:\n","            unique_words[word] = unique_words_count\n","            unique_words_count += 1\n","    \n","    # Iterate over all n-grams to collect all possible words\n","    for n_gram in n_grams:\n","        for word in n_gram:\n","            # [TODO] Add the word to the unique_words dictionary\n","            # hint: use the _add_to_unique_word function\n","            _add_to_unique_word(word)\n","    \n","    \n","    ## Step 3.2: Compute the transition matrix\n","    \n","    # In this part, we use `words_to_index` and `index_to_words` functions to convert between state name (words) and indices\n","    \n","    # Example:\n","    # If we have 3 unique words: 0 for \"apple\", 1 for \"banana\", 2 for \"cherry\", and n = 3,\n","    # then the state names are \"apple, apple\", \"apple, banana\", \"apple, cherry\", \"banana, apple\", \"banana, banana\", and so on.\n","    # In this case, the mapping from states to indices would be:\n","    # \"(apple, apple)\" <-> 0, \"(apple, banana)\" <-> 1, \"(apple, cherry)\" <-> 2, \"(banana, apple)\" <-> 3, and so on.\n","    \n","    # [TODO] Compute the number of states in Markov chain. Number of states is the number of unique words to the power of n-1\n","    unique_states_count = unique_words_count ** (n-1)\n","    \n","    # [TODO] Create transition matrix, filled with zeros (the size of the transition matrix is # states x # states)\n","    transition_matrix = np.zeros((unique_states_count, unique_states_count))\n","\n","    # Count the number of transitions from each state to another state\n","    for n_gram in n_grams:\n","        # [TODO] Find the indices of the states (from and to) (i.e. the first n-1 words and the last n-1 words)\n","        # hint: use words_to_index function\n","        state_from_index = words_to_index(n_gram[:-1], unique_words_count, unique_words)\n","        state_to_index = words_to_index(n_gram[1:],unique_words_count , unique_words)\n","\n","        # Increment the count of the transition from state_from to state_to\n","        transition_matrix[state_from_index][state_to_index] += 1\n","\n","    # Special case:\n","    # if the sum of number of transition of a state is zero, which means it has no information about the next state,\n","    # we set the probabilities of transitioning to all possible next states to 1 / # words\n","    \n","    # Check if any row sums to zero\n","    row_sums_is_zero = transition_matrix.sum(axis=1) == 0\n","    \n","    \n","    # Iterate over rows to find zero-sum rows\n","    first_word = list(unique_words.keys())[0]\n","    \n","    for idx, is_zero in enumerate(row_sums_is_zero):\n","        # If the row sum is zero\n","        if is_zero:\n","            # Convert the index to state representation\n","            state_name = index_to_words(idx, unique_words_count, n-1, unique_words)\n","            \n","            # Calculate the range of indices for the next posiible states\n","            possible_states_start_index = words_to_index(state_name[1:] + [first_word, ], unique_words_count, unique_words)\n","            possible_states_end_index = possible_states_start_index + unique_words_count\n","            \n","            # Set the number of transitions to 1 for the range\n","            transition_matrix[idx, possible_states_start_index : possible_states_end_index] = 1\n","\n","    ## Step 3.3: Normalize the transition matrix\n","    \n","    # The following steps is to normalize transition matrix so that the sum of each row is 1\n","    # (The reason is: By the definition of a Markov chain, the sum of each row should be 1)\n","    \n","    # [TODO] Compute the sum of each row\n","    # hint: use np.sum function and check if the axis parameter is set correctly\n","    row_sums =transition_matrix.sum(axis = 1)\n","    \n","    # [TODO] Divide the transition matrix by the sum of each row to make the sum of each row equals to 1\n","    transition_matrix =  transition_matrix / row_sums[:,np.newaxis]\n","\n","    return transition_matrix, unique_words"]},{"cell_type":"markdown","metadata":{"id":"rLiaaGcpveet"},"source":["### Step 4: Generate text"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":641,"status":"ok","timestamp":1725465504726,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"Bh7NXbk4veet"},"outputs":[],"source":["def generate(unique_words: Dict[str, int], transition_matrix: np.ndarray, start_from: Union[str, List[str]], n: int, length: int=10):\n","    \"\"\"\n","        Generate text using a Markov chain model.\n","\n","        Args:\n","            unique_states (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n","            transition_matrix (np.ndarray): A numpy array representing the transition probabilities between states.\n","            start_from (Union[str, List[str]]): The starting state(s) for text generation.\n","            n (int): The size of the grams.\n","            length (int, optional): The length of the generated text. Defaults to 10.\n","\n","        Returns:\n","            The generated text.\n","    \"\"\"\n","    # Generate text\n","    generated_words = start_from.copy() if type(start_from) is list else [start_from]\n","    \n","        \n","    # Assert if the number of start words does not equal to n-1\n","    assert len(generated_words) >= n-1, \"The number of start words should be greater than or equals to n-1 ({})\".format(n-1)\n","\n","    # Get the number of unique words    \n","    unique_words_count = len(unique_words)\n","    \n","    # [TODO] Get the number of unique states\n","    # hint: check step 3.2 in the compute_transition_matrix function\n","    unique_states_count = unique_words_count ** (n-1)\n","    # Generate the next words\n","    for _ in range(length):\n","        # [TODO] Get index of current states\n","        # hint: The current states (current words) is the last n-1 words in the generated text\n","        # hint: use words_to_index function\n","        current_words_index = words_to_index(generated_words[-(n-1):],unique_words_count, unique_words)\n","\n","        # [TODO] Get probability distribution for next state, using the information in the transition matrix\n","        probabilities = transition_matrix[current_words_index]\n","\n","        # Select next word based on probabilities, using np.random.choice function\n","        next_words_index = np.random.choice(unique_states_count, p=probabilities)\n","\n","        # [TODO] Decode the index and get the last word\n","        # hint: use index_to_words function\n","        next_word = index_to_words(next_words_index,unique_words_count,n-1,unique_words)[-1]\n","\n","        # Add next word to generated text\n","        generated_words.append(next_word)\n","\n","    # return generated string\n","    return ' '.join(generated_words)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"KkOugSjwveeu"},"outputs":[{"name":"stdout","output_type":"stream","text":["n grams <generator object ngrams at 0x000002065D601740>\n"]}],"source":["# [TODO] Change the text below and try different values of n\n","# text = \"i love cat. cat love me. But I love dog too and dog also love me\"\n","text = \"\"\"the cat sleeps on the warm mat. birds sing in tall trees. children play in the park nearby. \n","cars rush down busy streets. a cool breeze blows through open windows.\"\"\"\n","\n","n = 3\n","\n","# Process the text and generate the transition matrix\n","text = remove_unnecessary_characters(text)\n","n_grams = generate_n_grams(text, n)\n","transition_matrix, unique_words = compute_transition_matrix(n_grams)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"4MAL08Egveeu","outputId":"4b6e5c82-80d2-46f9-cde7-76f3047dae53"},"outputs":[{"name":"stdout","output_type":"stream","text":["===== The indices for unique states are: =====\n","the,the   : 0\n","the,cat   : 1\n","the,sleeps: 2\n","the,on    : 3\n","the,warm  : 4\n","the,mat   : 5\n","the,birds : 6\n","the,sing  : 7\n","the,in    : 8\n","the,tall  : 9\n","the,trees : 10\n","the,children: 11\n","the,play  : 12\n","the,park  : 13\n","the,nearby: 14\n","the,cars  : 15\n","the,rush  : 16\n","the,down  : 17\n","the,busy  : 18\n","the,streets: 19\n","...\n","\n","===== The transition matrix is (Shape of trasition matrix: (729, 729)): =====\n","[[0.03703704 0.03703704 0.03703704 ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," ...\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.03703704 0.03703704 0.03703704]]\n","\n"]}],"source":["# Print the transition matrix and unique states for obsevation\n","print(\"===== The indices for unique states are: =====\")\n","unique_words_count = len(unique_words)\n","for word_name in list(product(*[unique_words for _ in range(n-1)]))[:20]:\n","    print(f\"{','.join(word_name):10s}: {words_to_index(word_name, unique_words_count, unique_words)}\")\n","print(\"...\", end=\"\\n\\n\")\n","\n","print(\"===== The transition matrix is (Shape of trasition matrix: {}): =====\".format(transition_matrix.shape))\n","print(transition_matrix)\n","print()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"rnbUL1itveev","outputId":"ea20fe90-5d66-4c76-ae61-32287f57848c"},"outputs":[{"name":"stdout","output_type":"stream","text":["#1 (length=10): the cat sleeps on the warm mat birds sing in tall trees\n","#2 (length=15): birds sing in tall trees children play in the park nearby cars rush down busy streets a\n","#3 (length=20): children play in the park nearby cars rush down busy streets a cool breeze blows through open windows streets park open cat streets\n"]}],"source":["# [TODO] Write down 3 or more initial words and length of generated text to start the text generation\n","# experiments = [\n","#     # Each entry is (Initial words, the length of the generated text)\n","#     ('i love', 10), ('cat love', 15), ('love me', 30)\n","    \n","# ]\n","\n","experiments = [\n","    ('the cat', 10),\n","    ('birds sing', 15),\n","    ('children play in', 20)\n","]\n","\n","\n","for idx, (start_from, length) in enumerate(experiments, 1):\n","    start_from = start_from.split(\" \")\n","\n","    # Generate text using the transition matrix\n","    generated_text = generate(unique_words, transition_matrix, start_from, n, length=length)\n","    \n","    # Print out the generated text\n","    print(\"#{} (length={}): {}\".format(idx, length, generated_text))"]},{"cell_type":"markdown","metadata":{"id":"YrK8ourT6aE2"},"source":["### Answer the following questions"]},{"cell_type":"markdown","metadata":{"id":"nrUZ91rpvcPt"},"source":["* (a) Write a new text of at least 15 words as the input.  \n","    the cat sleeps on the warm mat. birds sing in tall trees. children play in the park nearby. cars rush down busy streets.        \n","    a cool breeze blows through open windows.\n","* (b) Run the program 3 times with different output length and different initial words. Show the outputs.\n","    * (Please modify the `experiments` at the cell above and run the cell)  \n","    \n","    #1 (length=10): the cat sleeps on the warm mat birds sing in tall trees  \n","    #2 (length=15): birds sing in tall trees children play in the park nearby cars rush down busy streets a  \n","    #3 (length=20): children play in the park nearby cars rush down busy streets a cool breeze blows through open windows streets park open cat streets  \n","    \n","* (c) Try different N of N-grams. How the N influences the output sequence?  \n","    The value of N in N-grams significantly influences the output sequence. As N increases, the generated text becomes more fluent and coherent, closely resembling natural human language. Larger N-grams capture more extended word sequences, preserving the original text's structure and context. Conversely, smaller N values produce more random and often incomprehensible text due to limited context. However, while larger N values enhance fluency, they may limit creativity by reproducing longer sequences from the original text. Smaller N values, despite reduced coherence, allow for more unexpected word combinations. The optimal N value often balances fluency and creativity, depending on the specific requirements of the text generation task.\n"]},{"cell_type":"markdown","metadata":{"id":"0Ej2DuDTveev"},"source":["## Problem 3\n","The Stationary Distribution of a Markov chain is a distribution of probabilities that remains unchanged after a transition from one state to another."]},{"cell_type":"markdown","metadata":{"id":"8EGILVlKvcPu"},"source":["### (a)  Ask an LLM (Large Language Model), such as ChatGPT, what are the applications of stationary distribution of a Markov chain. You need to show which prompts are used, and state how you verify the correctness of the results (output by LLMs).\n","\n","#### Prompt used:\n","What are the main applications of the stationary distribution of a Markov chain? Please provide several examples from different fields.\n","\n","#### LLM's response:\n","\n","1. Web Page Ranking (PageRank Algorithm):\n","   - Application: Search engine result ranking\n","   - Role: Web pages are viewed as states in a Markov chain, with links as transition probabilities. The stationary distribution represents the importance or weight of each web page.\n","\n","2. Genetic Sequence Analysis:\n","   - Application: Predicting gene structure in DNA sequences\n","   - Role: Using Hidden Markov Models, the stationary distribution helps identify coding and non-coding regions.\n","\n","3. Financial Market Modeling:\n","   - Application: Stock price prediction, risk assessment\n","   - Role: Market states are viewed as a Markov chain, with the stationary distribution reflecting long-term market trends.\n","\n","#### Verification methods:\n","1. Cross-validation: Consult reliable academic resources (textbooks, journal articles)\n","2. Logical consistency check: Ensure logical soundness and consistency with Markov chain principles\n","3. Case studies: Look for actual research or industrial application cases\n","\n","### (b) Ask an LLM, such as ChatGPT, what numerical method is the most efficient approach to compute the stationary distribution? You need to show which prompts are used, and state how you verify the correctness of the results (output by LLMs)\n","\n","#### Prompt used:\n","What numerical method is the most efficient approach to compute the stationary distribution?\n","\n","#### LLM's response:\n","\n","To determine the most efficient numerical method for computing the stationary distribution, we need to consider a few factors:\n","\n","- The size and sparsity of the transition matrix\n","- The desired accuracy\n","- The available computational resources\n","\n","Generally, for Markov chains with reasonably sized transition matrices, the power method is often considered one of the most efficient approaches. Here's a brief overview:\n","\n","Power Method:\n","- Simple to implement\n","- Works well for sparse matrices\n","- Converges relatively quickly for many practical problems\n","\n","However, for very large or ill-conditioned matrices, other methods may be more suitable:\n","\n","Iterative methods:\n","- Gauss-Seidel method\n","- Jacobi method\n","These can be faster for certain types of matrices\n","\n","Linear algebra approaches:\n","- Eigenvalue decomposition\n","- Solving linear systems\n","Can be very fast for smaller matrices, but may become computationally expensive for large ones\n","\n","The \"most efficient\" method can vary depending on the specific problem. For many practical applications, the power method offers a good balance of simplicity and efficiency. However, if you're dealing with very large or specialized problems, you might need to experiment with different methods to find the most efficient one for your specific case.\n","\n","#### Verification methods:\n","1. Software Package Analysis:\n","- Investigate popular numerical computation software packages (e.g., MATLAB, NumPy, SciPy) to see which methods they use as default for computing stationary distributions.\n","- Compare the choices made by these established software packages with the LLM's recommendations.\n","- Check the documentation of these packages for any performance comparisons or recommendations for different types of matrices.\n","2. Case Studies:\n","- Search for real-world applications where stationary distributions are computed.\n","- Examine which methods are commonly used in practice for different types of Markov chains.\n","- Compare the methods used in these case studies with those suggested by the LLM.\n","### (c) Implement the method suggested by the LLM. Use the transition matrix generated in question 2.a as an input to compute its stationary distribution.\n","[TODO: Implement the power method as suggested by the LLM]\n","def compute_stationary_distribution(transition_matrix):\n","    # Implement the power method here\n","    pass\n","\n","### (d) The theory of probability matrix is given in the textbook 6.8, Eigenvalues/eigenvector of nonegtive matrices. Read textbook 6.8 and ask two questions that you are curious most about it\n","\n","1. How does the concept of a regular Markov process contribute to our understanding of the long-term behavior of stochastic systems?\n","\n","2. In what way does Perron's theorem support the analysis of convergence properties in positive stochastic matrices, particularly in establishing the dominance of the eigenvalue λ = 1?"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"-VwWF0P1veev"},"outputs":[],"source":["def compute_stationary_distribution(transition_matrix: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","        Compute the stationary distribution of a Markov chain.\n","\n","        Parameters:\n","            transition_matrix (np.ndarray): A numpy array representing the transition probabilities between states.\n","        Returns:\n","            The stationary distribution of the Markov chain.\n","    \"\"\"\n","    # [TODO] Implement the function to compute the stationary distribution of a Markov chain, using the method suggested by the LLM\n","    n = transition_matrix.shape[0]\n","    stationary_distribution = np.ones(n) / n\n","    epsilon = 1e-8\n","    max_iterations = 1000\n","    for _ in range(max_iterations):\n","        new_distribution = stationary_distribution @ transition_matrix\n","        \n","        if np.allclose(new_distribution,stationary_distribution,epsilon):\n","            return new_distribution \n","        stationary_distribution = new_distribution\n","    return stationary_distribution"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"-owTykzFveev","outputId":"9ade2e5c-14a9-4043-c38d-85e3b20bd7a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.0001633 0.0001633 0.0001633 ... 0.0001754 0.0001754 0.0001754]\n"]}],"source":["# Compute the stationary distribution for the transition matrix obtained from the previous problem\n","stationary_distribution = compute_stationary_distribution(transition_matrix)\n","\n","# Convert the stationary distribution to float16 to prevent something like -1.2345678e-16 but it is actually 0\n","print(stationary_distribution.astype(np.float16))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
